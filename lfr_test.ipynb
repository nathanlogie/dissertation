{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2024-11-18T20:12:06.045447Z",
     "start_time": "2024-11-18T20:11:55.012624Z"
    }
   },
   "source": [
    "import pandas as pd\n",
    "from aif360.algorithms.preprocessing import LFR\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
    "from aif360.metrics import BinaryLabelDatasetMetric, ClassificationMetric\n",
    "from aif360.datasets import BinaryLabelDataset"
   ],
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:No module named 'fairlearn': ExponentiatedGradientReduction will be unavailable. To install, run:\n",
      "pip install 'aif360[Reductions]'\n",
      "WARNING:root:No module named 'fairlearn': GridSearchReduction will be unavailable. To install, run:\n",
      "pip install 'aif360[Reductions]'\n",
      "WARNING:root:No module named 'inFairness': SenSeI and SenSR will be unavailable. To install, run:\n",
      "pip install 'aif360[inFairness]'\n",
      "WARNING:root:No module named 'fairlearn': GridSearchReduction will be unavailable. To install, run:\n",
      "pip install 'aif360[Reductions]'\n"
     ]
    }
   ],
   "execution_count": 1
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-18T20:31:21.400398Z",
     "start_time": "2024-11-18T20:31:21.261038Z"
    }
   },
   "cell_type": "code",
   "source": [
    "filepath = \"datasets/processed_datasets/compass.csv\"\n",
    "sensitive_attribute = \"race\"\n",
    "target_column = \"two_year_recid\"\n",
    "\n",
    "\n",
    "df = pd.read_csv(filepath, header=0,skipinitialspace=True)\n",
    "target_name = \"predicted_\" + target_column\n",
    "df[sensitive_attribute] = LabelEncoder().fit_transform(df[sensitive_attribute])\n",
    "\n"
   ],
   "id": "9d219ef253ee191f",
   "outputs": [],
   "execution_count": 14
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-18T20:40:11.407738Z",
     "start_time": "2024-11-18T20:40:11.329208Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from sklearn.utils import resample\n",
    "\n",
    "y = df[target_column]\n",
    "X = df.drop(columns=[target_column])\n",
    "\n",
    "# Perform train-test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "\n",
    "# Combine X_train and y_train for balancing\n",
    "train_data = pd.concat([X_train.reset_index(drop=True), y_train.reset_index(drop=True)], axis=1)\n",
    "\n",
    "# Check class distribution before balancing\n",
    "print(\"Class distribution before balancing:\")\n",
    "print(train_data[target_column].value_counts())\n",
    "\n",
    "# Separate majority and minority classes\n",
    "majority_class = train_data[train_data[target_column] == 1]\n",
    "minority_class = train_data[train_data[target_column] == 0]\n",
    "\n",
    "# Upsample the minority class to balance the dataset\n",
    "if len(minority_class) < len(majority_class):\n",
    "    minority_class_upsampled = resample(minority_class,\n",
    "                                        replace=True,\n",
    "                                        n_samples=len(majority_class),\n",
    "                                        random_state=42)\n",
    "    # Combine majority and upsampled minority class\n",
    "    balanced_train_data = pd.concat([majority_class, minority_class_upsampled])\n",
    "else:\n",
    "    balanced_train_data = train_data\n",
    "\n",
    "# Shuffle the balanced dataset\n",
    "balanced_train_data = balanced_train_data.sample(frac=1, random_state=42).reset_index(drop=True)\n",
    "\n",
    "# Split balanced data into features and target\n",
    "X_train_balanced = balanced_train_data.drop(columns=[target_column])\n",
    "y_train_balanced = balanced_train_data[target_column]\n",
    "\n",
    "# Display class distribution after balancing\n",
    "print(\"Class distribution after balancing:\")\n",
    "print(y_train_balanced.value_counts())\n",
    "\n",
    "# Prepare the BinaryLabelDataset for training and testing\n",
    "train_dataset = BinaryLabelDataset(df=pd.concat([X_train_balanced.reset_index(drop=True), y_train_balanced.reset_index(drop=True)], axis=1),\n",
    "                                   label_names=[target_column],\n",
    "                                   protected_attribute_names=[sensitive_attribute])\n",
    "\n",
    "test_dataset = BinaryLabelDataset(df=pd.concat([X_test.reset_index(drop=True), y_test.reset_index(drop=True)], axis=1),\n",
    "                                  label_names=[target_column],\n",
    "                                  protected_attribute_names=[sensitive_attribute])\n",
    "\n",
    "print(\"Train and test datasets created successfully.\")"
   ],
   "id": "e421ef10618ffcb3",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Class distribution before balancing:\n",
      "two_year_recid\n",
      "1    2756\n",
      "0    2293\n",
      "Name: count, dtype: int64\n",
      "Class distribution after balancing:\n",
      "two_year_recid\n",
      "0    2756\n",
      "1    2756\n",
      "Name: count, dtype: int64\n",
      "Train and test datasets created successfully.\n"
     ]
    }
   ],
   "execution_count": 19
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-18T20:57:10.662857Z",
     "start_time": "2024-11-18T20:53:56.141761Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from collections import Counter\n",
    "\n",
    "y = df[target_column]\n",
    "X = df.drop(columns=[target_column])\n",
    "\n",
    "# Step 2: Perform train-test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "\n",
    "# Step 3: Combine X_train and y_train for balancing\n",
    "train_data = pd.concat([X_train.reset_index(drop=True), y_train.reset_index(drop=True)], axis=1)\n",
    "\n",
    "# Step 4: Check class distribution before balancing\n",
    "print(\"Class distribution before balancing:\", Counter(train_data[target_column]))\n",
    "\n",
    "# Step 5: Balancing the dataset with upsampling\n",
    "majority_class = train_data[train_data[target_column] == 0]\n",
    "minority_class = train_data[train_data[target_column] == 1]\n",
    "\n",
    "if len(minority_class) < len(majority_class):\n",
    "    # Upsample minority class\n",
    "    minority_class_upsampled = resample(minority_class, \n",
    "                                        replace=True,\n",
    "                                        n_samples=len(majority_class), \n",
    "                                        random_state=42)\n",
    "    # Combine majority class with upsampled minority class\n",
    "    balanced_train_data = pd.concat([majority_class, minority_class_upsampled])\n",
    "else:\n",
    "    balanced_train_data = train_data\n",
    "\n",
    "# Shuffle the balanced data\n",
    "balanced_train_data = balanced_train_data.sample(frac=1, random_state=42).reset_index(drop=True)\n",
    "\n",
    "# Step 6: Split balanced data into features and target\n",
    "X_train_balanced = balanced_train_data.drop(columns=[target_column])\n",
    "y_train_balanced = balanced_train_data[target_column]\n",
    "\n",
    "# Step 7: Create BinaryLabelDataset for balanced training and test data\n",
    "train_dataset = BinaryLabelDataset(df=pd.concat([X_train_balanced.reset_index(drop=True), y_train_balanced.reset_index(drop=True)], axis=1),\n",
    "                                   label_names=[target_column],\n",
    "                                   protected_attribute_names=[sensitive_attribute])\n",
    "\n",
    "test_dataset = BinaryLabelDataset(df=pd.concat([X_test.reset_index(drop=True), y_test.reset_index(drop=True)], axis=1),\n",
    "                                  label_names=[target_column],\n",
    "                                  protected_attribute_names=[sensitive_attribute])\n",
    "\n",
    "# Step 8: Verify label distribution after balancing\n",
    "print(\"Balanced Train Dataset Distribution:\", Counter(train_dataset.labels.ravel()))\n",
    "print(\"Test Dataset Distribution:\", Counter(test_dataset.labels.ravel()))\n",
    "\n",
    "# Step 9: Apply LFR Transformation\n",
    "from aif360.algorithms.preprocessing import LFR\n",
    "\n",
    "lfr = LFR(unprivileged_groups=[{sensitive_attribute: 0}],\n",
    "          privileged_groups=[{sensitive_attribute: 1}],\n",
    "          verbose=0,\n",
    "          seed=4048)\n",
    "\n",
    "# Fit LFR only if dataset has at least two classes\n",
    "if len(set(train_dataset.labels.ravel())) > 1:\n",
    "    lfr.fit(train_dataset)\n",
    "    train_dataset_transformed = lfr.transform(train_dataset)\n",
    "    test_dataset_transformed = lfr.transform(test_dataset)\n",
    "\n",
    "    # Extract transformed features and labels\n",
    "    X_train_transformed = train_dataset_transformed.features\n",
    "    y_train_transformed = train_dataset_transformed.labels.ravel()\n",
    "\n",
    "    X_test_transformed = test_dataset_transformed.features\n",
    "    y_test_transformed = test_dataset_transformed.labels.ravel()\n",
    "\n",
    "    # Verify label distribution post-transformation\n",
    "    print(\"Label Distribution After LFR Transformation (Training):\", Counter(y_train_transformed))\n",
    "    print(\"Label Distribution After LFR Transformation (Testing):\", Counter(y_test_transformed))"
   ],
   "id": "dfaf532316a3ae30",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Class distribution before balancing: Counter({1: 2756, 0: 2293})\n",
      "Balanced Train Dataset Distribution: Counter({np.float64(1.0): 2756, np.float64(0.0): 2293})\n",
      "Test Dataset Distribution: Counter({np.float64(1.0): 1207, np.float64(0.0): 958})\n",
      "Label Distribution After LFR Transformation (Training): Counter({np.float64(1.0): 5049})\n",
      "Label Distribution After LFR Transformation (Testing): Counter({np.float64(1.0): 2165})\n"
     ]
    }
   ],
   "execution_count": 24
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-18T20:48:23.767456Z",
     "start_time": "2024-11-18T20:48:23.578535Z"
    }
   },
   "cell_type": "code",
   "source": [
    "print(len(set(y_train_transformed)))\n",
    "model = make_pipeline(StandardScaler(), LogisticRegression(solver='liblinear', random_state=1))\n",
    "model.fit(X_train_transformed, y_train_transformed)\n",
    "y_pred = model.predict(X_test_transformed)\n"
   ],
   "id": "2f7a931766b35ed6",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "This solver needs samples of at least 2 classes in the data, but the data contains only one class: np.float64(1.0)",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mValueError\u001B[0m                                Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[22], line 3\u001B[0m\n\u001B[1;32m      1\u001B[0m \u001B[38;5;28mprint\u001B[39m(\u001B[38;5;28mlen\u001B[39m(\u001B[38;5;28mset\u001B[39m(y_train_transformed)))\n\u001B[1;32m      2\u001B[0m model \u001B[38;5;241m=\u001B[39m make_pipeline(StandardScaler(), LogisticRegression(solver\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mliblinear\u001B[39m\u001B[38;5;124m'\u001B[39m, random_state\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m1\u001B[39m))\n\u001B[0;32m----> 3\u001B[0m \u001B[43mmodel\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mfit\u001B[49m\u001B[43m(\u001B[49m\u001B[43mX_train_transformed\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43my_train_transformed\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m      4\u001B[0m y_pred \u001B[38;5;241m=\u001B[39m model\u001B[38;5;241m.\u001B[39mpredict(X_test_transformed)\n",
      "File \u001B[0;32m/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/sklearn/base.py:1473\u001B[0m, in \u001B[0;36m_fit_context.<locals>.decorator.<locals>.wrapper\u001B[0;34m(estimator, *args, **kwargs)\u001B[0m\n\u001B[1;32m   1466\u001B[0m     estimator\u001B[38;5;241m.\u001B[39m_validate_params()\n\u001B[1;32m   1468\u001B[0m \u001B[38;5;28;01mwith\u001B[39;00m config_context(\n\u001B[1;32m   1469\u001B[0m     skip_parameter_validation\u001B[38;5;241m=\u001B[39m(\n\u001B[1;32m   1470\u001B[0m         prefer_skip_nested_validation \u001B[38;5;129;01mor\u001B[39;00m global_skip_validation\n\u001B[1;32m   1471\u001B[0m     )\n\u001B[1;32m   1472\u001B[0m ):\n\u001B[0;32m-> 1473\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mfit_method\u001B[49m\u001B[43m(\u001B[49m\u001B[43mestimator\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/sklearn/pipeline.py:473\u001B[0m, in \u001B[0;36mPipeline.fit\u001B[0;34m(self, X, y, **params)\u001B[0m\n\u001B[1;32m    471\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_final_estimator \u001B[38;5;241m!=\u001B[39m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mpassthrough\u001B[39m\u001B[38;5;124m\"\u001B[39m:\n\u001B[1;32m    472\u001B[0m         last_step_params \u001B[38;5;241m=\u001B[39m routed_params[\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39msteps[\u001B[38;5;241m-\u001B[39m\u001B[38;5;241m1\u001B[39m][\u001B[38;5;241m0\u001B[39m]]\n\u001B[0;32m--> 473\u001B[0m         \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_final_estimator\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mfit\u001B[49m\u001B[43m(\u001B[49m\u001B[43mXt\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43my\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mlast_step_params\u001B[49m\u001B[43m[\u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mfit\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m]\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    475\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\n",
      "File \u001B[0;32m/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/sklearn/base.py:1473\u001B[0m, in \u001B[0;36m_fit_context.<locals>.decorator.<locals>.wrapper\u001B[0;34m(estimator, *args, **kwargs)\u001B[0m\n\u001B[1;32m   1466\u001B[0m     estimator\u001B[38;5;241m.\u001B[39m_validate_params()\n\u001B[1;32m   1468\u001B[0m \u001B[38;5;28;01mwith\u001B[39;00m config_context(\n\u001B[1;32m   1469\u001B[0m     skip_parameter_validation\u001B[38;5;241m=\u001B[39m(\n\u001B[1;32m   1470\u001B[0m         prefer_skip_nested_validation \u001B[38;5;129;01mor\u001B[39;00m global_skip_validation\n\u001B[1;32m   1471\u001B[0m     )\n\u001B[1;32m   1472\u001B[0m ):\n\u001B[0;32m-> 1473\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mfit_method\u001B[49m\u001B[43m(\u001B[49m\u001B[43mestimator\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/sklearn/linear_model/_logistic.py:1276\u001B[0m, in \u001B[0;36mLogisticRegression.fit\u001B[0;34m(self, X, y, sample_weight)\u001B[0m\n\u001B[1;32m   1270\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m effective_n_jobs(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mn_jobs) \u001B[38;5;241m!=\u001B[39m \u001B[38;5;241m1\u001B[39m:\n\u001B[1;32m   1271\u001B[0m         warnings\u001B[38;5;241m.\u001B[39mwarn(\n\u001B[1;32m   1272\u001B[0m             \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mn_jobs\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124m > 1 does not have any effect when\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m   1273\u001B[0m             \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m \u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124msolver\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124m is set to \u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mliblinear\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124m. Got \u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mn_jobs\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m   1274\u001B[0m             \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m = \u001B[39m\u001B[38;5;132;01m{}\u001B[39;00m\u001B[38;5;124m.\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;241m.\u001B[39mformat(effective_n_jobs(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mn_jobs))\n\u001B[1;32m   1275\u001B[0m         )\n\u001B[0;32m-> 1276\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mcoef_, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mintercept_, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mn_iter_ \u001B[38;5;241m=\u001B[39m \u001B[43m_fit_liblinear\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m   1277\u001B[0m \u001B[43m        \u001B[49m\u001B[43mX\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   1278\u001B[0m \u001B[43m        \u001B[49m\u001B[43my\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   1279\u001B[0m \u001B[43m        \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mC\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   1280\u001B[0m \u001B[43m        \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mfit_intercept\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   1281\u001B[0m \u001B[43m        \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mintercept_scaling\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   1282\u001B[0m \u001B[43m        \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mclass_weight\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   1283\u001B[0m \u001B[43m        \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mpenalty\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   1284\u001B[0m \u001B[43m        \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mdual\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   1285\u001B[0m \u001B[43m        \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mverbose\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   1286\u001B[0m \u001B[43m        \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mmax_iter\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   1287\u001B[0m \u001B[43m        \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mtol\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   1288\u001B[0m \u001B[43m        \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mrandom_state\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   1289\u001B[0m \u001B[43m        \u001B[49m\u001B[43msample_weight\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43msample_weight\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   1290\u001B[0m \u001B[43m    \u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   1291\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\n\u001B[1;32m   1293\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m solver \u001B[38;5;129;01min\u001B[39;00m [\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124msag\u001B[39m\u001B[38;5;124m\"\u001B[39m, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124msaga\u001B[39m\u001B[38;5;124m\"\u001B[39m]:\n",
      "File \u001B[0;32m/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/sklearn/svm/_base.py:1173\u001B[0m, in \u001B[0;36m_fit_liblinear\u001B[0;34m(X, y, C, fit_intercept, intercept_scaling, class_weight, penalty, dual, verbose, max_iter, tol, random_state, multi_class, loss, epsilon, sample_weight)\u001B[0m\n\u001B[1;32m   1171\u001B[0m     classes_ \u001B[38;5;241m=\u001B[39m enc\u001B[38;5;241m.\u001B[39mclasses_\n\u001B[1;32m   1172\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mlen\u001B[39m(classes_) \u001B[38;5;241m<\u001B[39m \u001B[38;5;241m2\u001B[39m:\n\u001B[0;32m-> 1173\u001B[0m         \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mValueError\u001B[39;00m(\n\u001B[1;32m   1174\u001B[0m             \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mThis solver needs samples of at least 2 classes\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m   1175\u001B[0m             \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m in the data, but the data contains only one\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m   1176\u001B[0m             \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m class: \u001B[39m\u001B[38;5;132;01m%r\u001B[39;00m\u001B[38;5;124m\"\u001B[39m \u001B[38;5;241m%\u001B[39m classes_[\u001B[38;5;241m0\u001B[39m]\n\u001B[1;32m   1177\u001B[0m         )\n\u001B[1;32m   1179\u001B[0m     class_weight_ \u001B[38;5;241m=\u001B[39m compute_class_weight(class_weight, classes\u001B[38;5;241m=\u001B[39mclasses_, y\u001B[38;5;241m=\u001B[39my)\n\u001B[1;32m   1180\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n",
      "\u001B[0;31mValueError\u001B[0m: This solver needs samples of at least 2 classes in the data, but the data contains only one class: np.float64(1.0)"
     ]
    }
   ],
   "execution_count": 22
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-18T20:18:03.965930Z",
     "start_time": "2024-11-18T20:18:03.920061Z"
    }
   },
   "cell_type": "code",
   "source": [
    "y_test_transformed = y_test.to_numpy()\n",
    "\n",
    "accuracy = accuracy_score(y_test_transformed, y_pred)\n",
    "precision = precision_score(y_test_transformed, y_pred)\n",
    "recall = recall_score(y_test_transformed, y_pred)\n",
    "f1 = f1_score(y_test_transformed, y_pred)\n",
    "\n",
    "X_test_copy = X_test.copy() \n",
    "X_test_copy[target_column] = y_test_transformed\n",
    "\n",
    "dataset_true = BinaryLabelDataset(df=X_test_copy,\n",
    "                                  label_names=[target_column],\n",
    "                                  protected_attribute_names=[sensitive_attribute])\n",
    "\n",
    "X_test_pred = X_test.copy()\n",
    "X_test_pred[target_column] = y_pred\n",
    "\n",
    "dataset_predicted = BinaryLabelDataset(df=X_test_pred,\n",
    "                                       label_names=[target_column],\n",
    "                                       protected_attribute_names=[sensitive_attribute])\n",
    "\n",
    "assert dataset_true.labels.shape == dataset_predicted.labels.shape, \\\n",
    "    f\"Shape mismatch: True labels {dataset_true.labels.shape}, Predicted labels {dataset_predicted.labels.shape}\"\n",
    "\n",
    "metric = BinaryLabelDatasetMetric(dataset_true,\n",
    "                                  privileged_groups=[{sensitive_attribute: 1}],\n",
    "                                  unprivileged_groups=[{sensitive_attribute: 0}])\n",
    "\n",
    "classification_metric = ClassificationMetric(dataset_true, dataset_predicted,\n",
    "                                             privileged_groups=[{sensitive_attribute: 1}],\n",
    "                                             unprivileged_groups=[{sensitive_attribute: 0}])\n",
    "\n",
    "disparate_impact = metric.disparate_impact()\n",
    "statistical_parity_diff = metric.statistical_parity_difference()\n",
    "\n",
    "ppv_privileged = classification_metric.positive_predictive_value(privileged=True)\n",
    "ppv_unprivileged = classification_metric.positive_predictive_value(privileged=False)\n",
    "ppv_parity = abs(ppv_privileged - ppv_unprivileged)\n",
    "\n",
    "fpr_privileged = classification_metric.false_positive_rate(privileged=True)\n",
    "fpr_unprivileged = classification_metric.false_positive_rate(privileged=False)\n",
    "fpr_parity = abs(fpr_privileged - fpr_unprivileged)\n",
    "\n",
    "results = {\n",
    "    \"Accuracy\": accuracy,\n",
    "    \"Precision\": precision,\n",
    "    \"Recall\": recall,\n",
    "    \"F1 Score\": f1,\n",
    "    \"Disparate Impact\": disparate_impact,\n",
    "    \"Statistical Parity Difference\": statistical_parity_diff,\n",
    "    \"PPV Parity\": ppv_parity,\n",
    "    \"FPR Parity\": fpr_parity\n",
    "}\n",
    "\n",
    "print(results)\n"
   ],
   "id": "e4ce8c1e583ce940",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Accuracy': 0.7548900430986849, 'Precision': np.float64(0.5747663551401869), 'Recall': np.float64(0.10780017528483786), 'F1 Score': np.float64(0.181549815498155), 'Disparate Impact': np.float64(0.3589947942816373), 'Statistical Parity Difference': np.float64(-0.2036429050424906), 'PPV Parity': np.float64(0.314696106362773), 'FPR Parity': np.float64(0.00022729688361621675)}\n"
     ]
    }
   ],
   "execution_count": 12
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
